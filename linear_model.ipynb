{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    },
    "colab": {
      "name": "linear_model.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HzDXmA0tvPx"
      },
      "source": [
        "# 0. Introduction\n",
        "\n",
        "In this notebook the final model for Day-Ahead electricity price forecasting is developed. Among raw data, this model feeds on precomputed bottleneck features. These bottleneck features are\n",
        "* Hourly price forecasts from a pure time series based model\n",
        "* Hourly wind and solar power generation for Germany from a renewable generation prediction model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QbkNxnkdt1jn",
        "outputId": "aa339b7f-77f6-4c86-b116-7d6963c909d4"
      },
      "source": [
        "!git clone https://github.com/farwacheema/DA-electricity-price-forecasting\n",
        "%cd DA-electricity-price-forecasting"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'DA-electricity-price-forecasting'...\n",
            "remote: Enumerating objects: 328, done.\u001b[K\n",
            "remote: Total 328 (delta 0), reused 0 (delta 0), pack-reused 328\u001b[K\n",
            "Receiving objects: 100% (328/328), 61.38 MiB | 28.58 MiB/s, done.\n",
            "Resolving deltas: 100% (144/144), done.\n",
            "/content/DA-electricity-price-forecasting\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KGZU-yuotvPy",
        "outputId": "889082bf-b6d7-44cd-9293-8b73c6b99424"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import datetime as dt\n",
        "from keras.models import load_model\n",
        "from sklearn.externals import joblib\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, median_absolute_error, r2_score\n",
        "from sklearn.linear_model import Lasso, LassoLars, LinearRegression, ElasticNet, Ridge, PassiveAggressiveRegressor, \\\n",
        "SGDRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.ensemble import AdaBoostRegressor, GradientBoostingRegressor, RandomForestRegressor, BaggingRegressor\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV \n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzBOPYNEtvPy"
      },
      "source": [
        "# 1. Input data\n",
        "\n",
        "\n",
        "First, we load the historical price dataset, the variable we want to predict with the final model. Additional data in the form of hourly generation per production, hourly forecasted and actual load and renewable generation forcasts are also available. True generation levels as well as forecasts are available on the transparency platform of the German Transmission System Operators. The final model will rely only on load forecasts and bottleneck features from previous models, since these inputs are the only ones available in time for the final prediction in a real world setting. The forecasts and true generation levels are not published prior to market closing and therefore irrlevant for price predictions The other datasets are only considered to give an indication about the model performance under _perfect conditions_. By including this data we are able to identify the margin for improvement of a model that is based on imperfect information. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPchP2MttvPy"
      },
      "source": [
        "# load historical Day-Ahead prices\n",
        "price_data = pd.read_csv('./raw_data/EPEX_spot_DA_auction_hour_prices_20070720-20170831.csv', parse_dates=True,\n",
        "                        index_col=0)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kfazVENtvPy"
      },
      "source": [
        "# load generation forecasts, actual generation per production type, forecasted and actual load in Germany\n",
        "actual = pd.read_csv('./processed_data/20150101-20170830-gen_per_prod_type.csv', parse_dates=True, index_col=0)\n",
        "forecast = pd.read_csv('./processed_data/20150101-20170830-forecast_load_renewable_gen.csv', parse_dates=True, index_col=0)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4iG_xeyFtvPy"
      },
      "source": [
        "# sum_forecasts contains solar forecasts and wind power forecasts which are available individually. We can drop\n",
        "# this dataset\n",
        "forecast.drop('sum_forecast', axis=1, inplace=True)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeYgRvBitvPy"
      },
      "source": [
        "# load the bottleneck features from the timeseries model\n",
        "timeseries_bottleneck = pd.read_csv('./bottleneck_features/bnf_timeseries.csv', parse_dates=True, index_col=0)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W2cnPP2ctvPy"
      },
      "source": [
        "# load the bottleneck features from the renewable generation model\n",
        "solar_bottleneck = pd.read_csv('./bottleneck_features/bnf_solar.csv', parse_dates=True, index_col=0)\n",
        "wind_bottleneck = pd.read_csv('./bottleneck_features/bnf_wind.csv', parse_dates=True, index_col=0)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-7Q-yo3tvPy"
      },
      "source": [
        "Historical generation, generation forecasts, loads and forecasted loads are available in quarter hour resolution. Since bottleneck features as well as Day-Ahead prices are given in an hourly resolution. Therefore, we resample the generation data to hourly intervals by taking the mean value over all quarter hour intervals of one individual hour (the unit of the resampled data is power, not energy. Thus averaging over all quarter hours is legitimate). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBMT3dGWtvPz"
      },
      "source": [
        "# resample input data\n",
        "actual = actual.resample('1H').mean()\n",
        "forecast = forecast.resample('1H').mean()"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oecWc645tvPz"
      },
      "source": [
        "All input datasets are merged into one big dataframe in order to align the indices and exclude missing data for all inputs simultaneously. This ensures consistent indexing among datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-P89050tvPz"
      },
      "source": [
        "# concatenate data \n",
        "features = pd.concat([actual, forecast, timeseries_bottleneck, solar_bottleneck, wind_bottleneck], axis=1)\n",
        "\n",
        "# drop rows containing NaN values\n",
        "features.dropna(inplace=True)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vknAcbBNtvPz"
      },
      "source": [
        "# ensure that features and labels have the same indexing by computing the index intersection of features and labels\n",
        "index = features.index.intersection(price_data.index)\n",
        "\n",
        "# pick features and labels with an index included in the intersection\n",
        "features = features.loc[index]\n",
        "labels = price_data.loc[index]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fHzsfkNYtvPz",
        "outputId": "0ec8099a-da49-458a-b827-aaeca139450f"
      },
      "source": [
        "# double check feature and label indexing\n",
        "labels.index.difference(features.index)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatetimeIndex([], dtype='datetime64[ns]', freq=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SAbBVppmtvQL",
        "outputId": "6ffb3030-76e2-4bbd-862b-5047d54ba2c7"
      },
      "source": [
        "features.index.difference(labels.index)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatetimeIndex([], dtype='datetime64[ns]', freq=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EPKl7SgDtvQM",
        "outputId": "c38ceaa8-849f-4ee3-9a48-5cdd6761e290"
      },
      "source": [
        "# print names of the included datasets\n",
        "features.columns"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['biomass', 'brown_coal', 'hard_coal', 'wind_offshore', 'pumped_hydro',\n",
              "       'solar', 'river_hydro', 'wind_onshore', 'nuclear', 'other',\n",
              "       'load_forecast', 'load_true', 'solar_forecast', 'offshore_forecast',\n",
              "       'onshore_forecast', 'timeseries_pred', 'solar_bottleneck',\n",
              "       'wind_bottleneck'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "rEX3XFN5tvQM",
        "outputId": "b731b475-e285-4307-dacc-062c82e6b0ac"
      },
      "source": [
        "features.head()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>biomass</th>\n",
              "      <th>brown_coal</th>\n",
              "      <th>hard_coal</th>\n",
              "      <th>wind_offshore</th>\n",
              "      <th>pumped_hydro</th>\n",
              "      <th>solar</th>\n",
              "      <th>river_hydro</th>\n",
              "      <th>wind_onshore</th>\n",
              "      <th>nuclear</th>\n",
              "      <th>other</th>\n",
              "      <th>load_forecast</th>\n",
              "      <th>load_true</th>\n",
              "      <th>solar_forecast</th>\n",
              "      <th>offshore_forecast</th>\n",
              "      <th>onshore_forecast</th>\n",
              "      <th>timeseries_pred</th>\n",
              "      <th>solar_bottleneck</th>\n",
              "      <th>wind_bottleneck</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2015-01-01 01:00:00</th>\n",
              "      <td>4261.00</td>\n",
              "      <td>15364.75</td>\n",
              "      <td>1929.75</td>\n",
              "      <td>516.25</td>\n",
              "      <td>409.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2617.00</td>\n",
              "      <td>8367.5</td>\n",
              "      <td>11086.25</td>\n",
              "      <td>4743.50</td>\n",
              "      <td>46952.50</td>\n",
              "      <td>47032.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>598.25</td>\n",
              "      <td>8161.75</td>\n",
              "      <td>16.366163</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12577.362305</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-01-01 02:00:00</th>\n",
              "      <td>4295.50</td>\n",
              "      <td>14852.75</td>\n",
              "      <td>1824.00</td>\n",
              "      <td>514.00</td>\n",
              "      <td>632.75</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2578.75</td>\n",
              "      <td>8604.0</td>\n",
              "      <td>11026.25</td>\n",
              "      <td>4836.50</td>\n",
              "      <td>45751.50</td>\n",
              "      <td>45619.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>599.50</td>\n",
              "      <td>8324.75</td>\n",
              "      <td>13.697455</td>\n",
              "      <td>0.0</td>\n",
              "      <td>11986.717773</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-01-01 03:00:00</th>\n",
              "      <td>4313.75</td>\n",
              "      <td>14111.00</td>\n",
              "      <td>1959.00</td>\n",
              "      <td>517.75</td>\n",
              "      <td>558.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2545.25</td>\n",
              "      <td>8617.0</td>\n",
              "      <td>11027.75</td>\n",
              "      <td>4840.25</td>\n",
              "      <td>45306.25</td>\n",
              "      <td>44253.75</td>\n",
              "      <td>0.0</td>\n",
              "      <td>603.75</td>\n",
              "      <td>8440.25</td>\n",
              "      <td>10.958999</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12911.827148</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-01-01 04:00:00</th>\n",
              "      <td>4308.50</td>\n",
              "      <td>14149.00</td>\n",
              "      <td>2012.25</td>\n",
              "      <td>519.75</td>\n",
              "      <td>602.75</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2557.75</td>\n",
              "      <td>8707.5</td>\n",
              "      <td>10962.25</td>\n",
              "      <td>4820.75</td>\n",
              "      <td>45423.00</td>\n",
              "      <td>43765.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>605.25</td>\n",
              "      <td>8621.25</td>\n",
              "      <td>10.259408</td>\n",
              "      <td>0.0</td>\n",
              "      <td>12743.263672</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-01-01 05:00:00</th>\n",
              "      <td>4304.00</td>\n",
              "      <td>13509.50</td>\n",
              "      <td>1753.50</td>\n",
              "      <td>520.00</td>\n",
              "      <td>629.25</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2554.75</td>\n",
              "      <td>8775.5</td>\n",
              "      <td>10696.00</td>\n",
              "      <td>4958.00</td>\n",
              "      <td>45701.50</td>\n",
              "      <td>43589.50</td>\n",
              "      <td>0.0</td>\n",
              "      <td>611.25</td>\n",
              "      <td>8825.75</td>\n",
              "      <td>11.003360</td>\n",
              "      <td>0.0</td>\n",
              "      <td>13013.442383</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                     biomass  brown_coal  ...  solar_bottleneck  wind_bottleneck\n",
              "2015-01-01 01:00:00  4261.00    15364.75  ...               0.0     12577.362305\n",
              "2015-01-01 02:00:00  4295.50    14852.75  ...               0.0     11986.717773\n",
              "2015-01-01 03:00:00  4313.75    14111.00  ...               0.0     12911.827148\n",
              "2015-01-01 04:00:00  4308.50    14149.00  ...               0.0     12743.263672\n",
              "2015-01-01 05:00:00  4304.00    13509.50  ...               0.0     13013.442383\n",
              "\n",
              "[5 rows x 18 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "bRgzmSeZtvQR",
        "outputId": "cfb42524-e2a7-41ef-d4d9-98b031d56d6d"
      },
      "source": [
        "labels.head()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>DA_price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2015-01-01 01:00:00</th>\n",
              "      <td>18.29</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-01-01 02:00:00</th>\n",
              "      <td>16.04</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-01-01 03:00:00</th>\n",
              "      <td>14.60</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-01-01 04:00:00</th>\n",
              "      <td>14.95</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2015-01-01 05:00:00</th>\n",
              "      <td>14.50</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                     DA_price\n",
              "2015-01-01 01:00:00     18.29\n",
              "2015-01-01 02:00:00     16.04\n",
              "2015-01-01 03:00:00     14.60\n",
              "2015-01-01 04:00:00     14.95\n",
              "2015-01-01 05:00:00     14.50"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "czxIab91tvQR"
      },
      "source": [
        "# 2. Developing the final prediction model\n",
        "\n",
        "Next, different regression models feeding on different subsets of the data are developed and evaluated. We will include data that is originally not available for our prediction in a real world setting in order to give a benchmark on the theoretical optimum of the modeling approach. This upper boundary serves as a reference for how far our model is from the 'perfect information' case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHfKx81xtvQR"
      },
      "source": [
        "def apply_predictor(predictor, features, labels, modelname='Some model'):\n",
        "        \n",
        "    \"\"\"Perform a train-test split. Fit a predictor on the training set and evaluate error metric on the test\n",
        "    set.\"\"\"\n",
        "    \n",
        "    # perform randomized train-test split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=0)\n",
        "    \n",
        "    # fit predictor and predict for test set\n",
        "    predictor.fit(X_train, y_train)\n",
        "    pred = predictor.predict(X_test)\n",
        "    \n",
        "    # compute performance metrics\n",
        "    mae = mean_absolute_error(y_test, pred)\n",
        "    mse = mean_squared_error(y_test, pred)\n",
        "    med = median_absolute_error(y_test, pred)\n",
        "    medape = np.median(np.abs((y_test - pred) / y_test) * 100)\n",
        "    rmse = np.sqrt(mse)\n",
        "    max_miss = max(abs(y_test - pred))\n",
        "    r2 = r2_score(y_test, pred)\n",
        "    \n",
        "    print('{7:s} mean absolute error: {0:.2f}, median absolute error: {1:.2f}, mean squared error {2:.2f}, '\n",
        "      'root mean squared error: {3:.2f}, median absolute percentage error: {4:.2f},  maximum deviation: {5:.2f},'\\\n",
        "      'r2 score: {6:.2f}'\\\n",
        "      .format(mae, med, mse, rmse, medape, max_miss, r2, modelname))\n",
        "    \n",
        "    return"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQT7shd9tvQR"
      },
      "source": [
        "## 2.1 Defining input feature sets "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWQQlEv7tvQS"
      },
      "source": [
        "**Idea:**\n",
        "\n",
        "Different inputs might be fed into the regression model. The following input categories together with the hypotheses connected to them will be used and compared:\n",
        "1. **'Perfect information':** Include all true major generation levels and the true load for the day of price prediction. Do not include any bottleneck features, especially not from the timeseries model. These inputs will serve as a reference point for the added value of the time series model.\n",
        "2. **'Perfect information with time series bottleneck features':** By how much does including the precomputed time series based prediction improve overall model performance?\n",
        "3. **'External renewable generation forecasts and true regular generation levels':** Are true renewable generation levels or renewable generation forecasts better inputs for price precition?\n",
        "4. **'External renewable forecasts - added benefit of time series model':** Same as 2., true renewable generation levels are replaced by external renewable forcasts.\n",
        "5. **'True renewable generation levels and timeseries bottleneck features':** Perfect information about renewable generation. Serves as benchmark for renewable bottleneck features.\n",
        "6. **'External renewable generation forecasts and timeseries bottleneck features':** Second benchmark for renewable bottleneck features considering external forecasts instead of true renewable generation.\n",
        "7. **'Final model':** All inputs that are available at before market closing, either external or precomputed by our own models are included. This is the 'real' prediction with information that would be available in a real world setting.\n",
        "8. **'Renewable bottleneck without timeseries bottleneck:'** Benchmark for the benefit of the timeseries model in an actual deployment scenario.|"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qT_C4ST6tvQS"
      },
      "source": [
        "# define input features to be used in the different input subsets\n",
        "cols1 = ['biomass', 'brown_coal', 'hard_coal', 'wind_offshore', 'pumped_hydro',\n",
        "       'solar', 'river_hydro', 'wind_onshore', 'nuclear', 'other', 'load_true']\n",
        "\n",
        "cols2 = ['biomass', 'brown_coal', 'hard_coal', 'wind_offshore', 'pumped_hydro',\n",
        "       'solar', 'river_hydro', 'wind_onshore', 'nuclear', 'other', 'load_true',\n",
        "       'timeseries_pred']\n",
        "\n",
        "cols3 = ['biomass', 'brown_coal', 'hard_coal', 'pumped_hydro', 'river_hydro',\n",
        "        'nuclear', 'other', 'load_forecast', 'solar_forecast', 'offshore_forecast',\n",
        "        'onshore_forecast']\n",
        "\n",
        "cols4 = ['biomass', 'brown_coal', 'hard_coal', 'pumped_hydro', 'river_hydro',\n",
        "        'nuclear', 'other', 'load_forecast', 'solar_forecast', 'offshore_forecast',\n",
        "        'onshore_forecast', 'timeseries_pred']\n",
        "\n",
        "cols5 = ['wind_offshore', 'solar', 'wind_onshore', 'load_forecast', 'timeseries_pred']\n",
        "\n",
        "cols6 = ['load_forecast', 'solar_forecast', 'offshore_forecast', 'onshore_forecast',\n",
        "        'timeseries_pred']\n",
        "\n",
        "cols7 = ['load_forecast', 'solar_bottleneck', 'wind_bottleneck', 'timeseries_pred']\n",
        "\n",
        "cols8 = ['load_forecast', 'solar_bottleneck', 'wind_bottleneck']\n",
        "\n",
        "input_variations = [cols1, cols2, cols3, cols4, cols5, cols6, cols7, cols8]"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2Xs8T_ntvQS"
      },
      "source": [
        "Now we can evaluate all different feature combinations and compare the results by applying a simple Linear Regression model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_OQCeIfstvQS",
        "outputId": "10448ae7-a85d-4856-d377-e7324b458c16"
      },
      "source": [
        "for i, curr_feat in enumerate(input_variations):\n",
        "    \n",
        "    # evaluate current feature selection and print mean absolute error\n",
        "    apply_predictor(LinearRegression(), features[curr_feat], labels.to_numpy().squeeze(), 'Feature set {}'.format(i+1))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Feature set 1 mean absolute error: 3.57, median absolute error: 2.79, mean squared error 22.95, root mean squared error: 4.79, median absolute percentage error: 8.88,  maximum deviation: 29.10,r2 score: 0.85\n",
            "Feature set 2 mean absolute error: 3.38, median absolute error: 2.71, mean squared error 20.22, root mean squared error: 4.50, median absolute percentage error: 8.78,  maximum deviation: 25.29,r2 score: 0.87\n",
            "Feature set 3 mean absolute error: 3.40, median absolute error: 2.61, mean squared error 21.27, root mean squared error: 4.61, median absolute percentage error: 8.30,  maximum deviation: 29.46,r2 score: 0.87\n",
            "Feature set 4 mean absolute error: 3.28, median absolute error: 2.55, mean squared error 19.26, root mean squared error: 4.39, median absolute percentage error: 8.18,  maximum deviation: 24.39,r2 score: 0.88\n",
            "Feature set 5 mean absolute error: 3.66, median absolute error: 2.88, mean squared error 23.79, root mean squared error: 4.88, median absolute percentage error: 9.14,  maximum deviation: 22.63,r2 score: 0.85\n",
            "Feature set 6 mean absolute error: 3.64, median absolute error: 2.89, mean squared error 23.35, root mean squared error: 4.83, median absolute percentage error: 9.11,  maximum deviation: 21.96,r2 score: 0.85\n",
            "Feature set 7 mean absolute error: 3.74, median absolute error: 2.90, mean squared error 24.67, root mean squared error: 4.97, median absolute percentage error: 9.23,  maximum deviation: 23.38,r2 score: 0.84\n",
            "Feature set 8 mean absolute error: 4.30, median absolute error: 3.46, mean squared error 32.71, root mean squared error: 5.72, median absolute percentage error: 11.13,  maximum deviation: 33.62,r2 score: 0.79\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8NIPon6ctvQS"
      },
      "source": [
        "Linear Regression is a very simple model. We can apply a more sophisticated algorithm in order to push the limits of predictive power a little further. Gradient Boosting generally shows a very good performance in all kinds of machine learing settings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5AgcRPEItvQS",
        "outputId": "c09e5a66-63c6-4ea5-8c11-bda0ec881c7a"
      },
      "source": [
        "for i, curr_feat in enumerate(input_variations):\n",
        "    \n",
        "    # evaluate current feature selection and print mean absolute error\n",
        "    apply_predictor(GradientBoostingRegressor(), features[curr_feat], labels.to_numpy().squeeze(), 'Feature set {}'.format(i+1))\n",
        "    "
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Feature set 1 mean absolute error: 3.15, median absolute error: 2.31, mean squared error 17.99, root mean squared error: 4.24, median absolute percentage error: 7.93,  maximum deviation: 22.73,r2 score: 0.89\n",
            "Feature set 2 mean absolute error: 3.05, median absolute error: 2.28, mean squared error 16.67, root mean squared error: 4.08, median absolute percentage error: 7.45,  maximum deviation: 21.85,r2 score: 0.89\n",
            "Feature set 3 mean absolute error: 3.05, median absolute error: 2.22, mean squared error 16.98, root mean squared error: 4.12, median absolute percentage error: 7.41,  maximum deviation: 24.85,r2 score: 0.89\n",
            "Feature set 4 mean absolute error: 2.94, median absolute error: 2.22, mean squared error 15.56, root mean squared error: 3.94, median absolute percentage error: 7.24,  maximum deviation: 20.96,r2 score: 0.90\n",
            "Feature set 5 mean absolute error: 3.53, median absolute error: 2.62, mean squared error 22.55, root mean squared error: 4.75, median absolute percentage error: 8.30,  maximum deviation: 25.23,r2 score: 0.86\n",
            "Feature set 6 mean absolute error: 3.45, median absolute error: 2.63, mean squared error 21.62, root mean squared error: 4.65, median absolute percentage error: 8.41,  maximum deviation: 26.12,r2 score: 0.86\n",
            "Feature set 7 mean absolute error: 3.57, median absolute error: 2.73, mean squared error 23.11, root mean squared error: 4.81, median absolute percentage error: 8.67,  maximum deviation: 24.95,r2 score: 0.85\n",
            "Feature set 8 mean absolute error: 4.19, median absolute error: 3.32, mean squared error 30.89, root mean squared error: 5.56, median absolute percentage error: 10.81,  maximum deviation: 29.26,r2 score: 0.80\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "649Ig5TMtvQT"
      },
      "source": [
        "**Analysis:**\n",
        "\n",
        "The results contain some very interesting implications about the problem structure and the importance of some features compared to others. The main points are:\n",
        "\n",
        "1. The best input features to our problem would be category 4, which are true generation, external renewable _forecasts_ and timeseries bottleneck features. These inputs yield better results than using the _true renewable generation_ inputs. This sounds counterintuitive in the first moment. Why are forecasted prediction levels better inputs than actual prediction levels? Thinking twice, the reason for that behavior is quite obvious. Day-Ahead prices are determined by _expectations_ of the market participants since prices are fixed before actual generation is known. The external forcasts are a representation of these expectations, aggregated by the TSOs after market clearing. Thus, they are a better description of the bidding strategies of all market participants than the actual generation levels. Hence, it would be better to predict _expected_ generation instead of _true_ generation as inputs for a price prediction model. This finding legitimates the usage of historical weather data for modeling actual generation levels, as we did with our renewable generation model. The model would perform even better if we fed it with weather forecasts instead of weather measurements. This assumes that the market participants base their individual expectations on similar weather forecasts than we use for the model. Forecasts from the German National Weather Service are an example for data with enough 'authority' to fulfill this assumption. Since our renewable generation model is able to predict real generation from real weather it seems fair to assume that it is able to predict expected generation based on expected weather conditions (forecasts). This hypothesis is supported by tests on category 5 and 6 (renewable forecasts vs renewable actual levels together with load forecasts and bottleneck features).\n",
        "2. The comparison of results for category 1 vs 2 and 3 vs 4 (3 and 4 include the time series bottleneck features) show that our time series model features are helpful for the overall model precision.\n",
        "3. The final model performance is actually not that far from perfect information. With a mean absolute error of 3.57€ per MWh is comparable to the benchmarks with external renewable forecasts (category 6, 3.45€ per MWh) and true renewable generation data (category 5, 3.53€ per MWh). The information that we would need to improve model performance substantially are estimations of non-renewable generation levels (e.g. brown coal, hard coal or nuclear). It might be possible to model these factors in additional models but this is out of the scope of this project.\n",
        "4. Including the time series bottleneck features is quite important for the final model, as the comparison of category 7 and 8 shows. The effect is significantly stronger than in the scenarios where we have additional features available. The time series features most likely include some information that otherwise would come from the generation level features. Since we are not considering them explicitly, including the time series model is quite crucial for the end result. \n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRGClPaWtvQT"
      },
      "source": [
        "## 2.2 Optimizing model parameter inputs via grid search cross validation\n",
        "\n",
        "In total, the performance of our final model is quite promising! We might be able to squeeze out a little more performance by tuning the hyperparameters of the gradient boosting algorithm. A grid search cross validation should help to find the optimal parametrization for our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyOAd3mjtvQT"
      },
      "source": [
        "> **Caution:** The grid search might take some time. Expect ~6 hours of runtime."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w5GPNmTvtvQT",
        "outputId": "308e77bf-a599-43de-d351-ee2a11666e1e"
      },
      "source": [
        "# define grid search parameters\n",
        "parameters = {'loss':('ls', 'lad', 'huber'), 'learning_rate':[0.05, 0.1, 0.3], 'n_estimators':[100, 1000, 5000],\n",
        "              'min_samples_split':[2, 10, 20], 'max_depth':[3, 5, 10]}\n",
        "\n",
        "# initialize predictor\n",
        "predictor = GradientBoostingRegressor(random_state=7)\n",
        "\n",
        "# initialize grid search instance\n",
        "clf = GridSearchCV(predictor, parameters, verbose=2)\n",
        "\n",
        "# iterate over parameter combinations to find the best parametrization\n",
        "clf.fit(features[cols7].to_numpy(), labels.to_numpy().squeeze())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 243 candidates, totalling 1215 fits\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=2, n_estimators=100 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=2, n_estimators=100, total=   0.9s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=2, n_estimators=100 \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.9s remaining:    0.0s\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[CV]  learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=2, n_estimators=100, total=   0.9s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=2, n_estimators=100 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=2, n_estimators=100, total=   0.9s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=2, n_estimators=100 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=2, n_estimators=100, total=   0.9s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=2, n_estimators=100 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=2, n_estimators=100, total=   0.9s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=2, n_estimators=1000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=2, n_estimators=1000, total=   8.7s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=2, n_estimators=1000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=2, n_estimators=1000, total=   8.6s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=2, n_estimators=1000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=2, n_estimators=1000, total=   8.6s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=2, n_estimators=1000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=2, n_estimators=1000, total=   8.7s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=2, n_estimators=1000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=2, n_estimators=1000, total=   8.8s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=2, n_estimators=5000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=2, n_estimators=5000, total=  43.6s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=2, n_estimators=5000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=2, n_estimators=5000, total=  43.1s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=2, n_estimators=5000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=2, n_estimators=5000, total=  43.1s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=2, n_estimators=5000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=2, n_estimators=5000, total=  43.6s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=2, n_estimators=5000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=2, n_estimators=5000, total=  44.2s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=10, n_estimators=100 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=10, n_estimators=100, total=   0.9s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=10, n_estimators=100 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=10, n_estimators=100, total=   0.9s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=10, n_estimators=100 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=10, n_estimators=100, total=   0.9s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=10, n_estimators=100 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=10, n_estimators=100, total=   0.9s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=10, n_estimators=100 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=10, n_estimators=100, total=   0.9s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=10, n_estimators=1000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=10, n_estimators=1000, total=   8.7s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=10, n_estimators=1000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=10, n_estimators=1000, total=   8.6s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=10, n_estimators=1000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=10, n_estimators=1000, total=   8.6s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=10, n_estimators=1000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=10, n_estimators=1000, total=   8.6s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=10, n_estimators=1000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=10, n_estimators=1000, total=   8.7s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=10, n_estimators=5000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=10, n_estimators=5000, total=  43.7s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=10, n_estimators=5000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=10, n_estimators=5000, total=  43.0s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=10, n_estimators=5000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=10, n_estimators=5000, total=  42.6s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=10, n_estimators=5000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=10, n_estimators=5000, total=  43.5s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=10, n_estimators=5000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=10, n_estimators=5000, total=  44.1s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=20, n_estimators=100 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=20, n_estimators=100, total=   0.9s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=20, n_estimators=100 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=20, n_estimators=100, total=   0.9s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=20, n_estimators=100 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=20, n_estimators=100, total=   0.9s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=20, n_estimators=100 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=20, n_estimators=100, total=   0.9s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=20, n_estimators=100 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=20, n_estimators=100, total=   0.9s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=20, n_estimators=1000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=20, n_estimators=1000, total=   8.8s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=20, n_estimators=1000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=20, n_estimators=1000, total=   8.7s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=20, n_estimators=1000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=20, n_estimators=1000, total=   8.7s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=20, n_estimators=1000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=20, n_estimators=1000, total=   8.7s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=20, n_estimators=1000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=20, n_estimators=1000, total=   8.8s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=20, n_estimators=5000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=20, n_estimators=5000, total=  43.7s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=20, n_estimators=5000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=20, n_estimators=5000, total=  43.1s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=20, n_estimators=5000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=20, n_estimators=5000, total=  42.8s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=20, n_estimators=5000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=20, n_estimators=5000, total=  43.4s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=20, n_estimators=5000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=3, min_samples_split=20, n_estimators=5000, total=  43.9s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=2, n_estimators=100 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=2, n_estimators=100, total=   1.3s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=2, n_estimators=100 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=2, n_estimators=100, total=   1.3s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=2, n_estimators=100 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=2, n_estimators=100, total=   1.3s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=2, n_estimators=100 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=2, n_estimators=100, total=   1.4s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=2, n_estimators=100 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=2, n_estimators=100, total=   1.4s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=2, n_estimators=1000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=2, n_estimators=1000, total=  14.0s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=2, n_estimators=1000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=2, n_estimators=1000, total=  13.8s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=2, n_estimators=1000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=2, n_estimators=1000, total=  13.8s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=2, n_estimators=1000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=2, n_estimators=1000, total=  14.0s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=2, n_estimators=1000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=2, n_estimators=1000, total=  14.2s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=2, n_estimators=5000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=2, n_estimators=5000, total= 1.2min\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=2, n_estimators=5000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=2, n_estimators=5000, total= 1.2min\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=2, n_estimators=5000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=2, n_estimators=5000, total= 1.2min\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=2, n_estimators=5000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=2, n_estimators=5000, total= 1.2min\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=2, n_estimators=5000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=2, n_estimators=5000, total= 1.2min\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=10, n_estimators=100 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=10, n_estimators=100, total=   1.3s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=10, n_estimators=100 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=10, n_estimators=100, total=   1.3s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=10, n_estimators=100 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=10, n_estimators=100, total=   1.3s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=10, n_estimators=100 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=10, n_estimators=100, total=   1.4s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=10, n_estimators=100 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=10, n_estimators=100, total=   1.4s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=10, n_estimators=1000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=10, n_estimators=1000, total=  14.1s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=10, n_estimators=1000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=10, n_estimators=1000, total=  13.8s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=10, n_estimators=1000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=10, n_estimators=1000, total=  13.7s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=10, n_estimators=1000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=10, n_estimators=1000, total=  14.0s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=10, n_estimators=1000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=10, n_estimators=1000, total=  14.1s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=10, n_estimators=5000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=10, n_estimators=5000, total= 1.2min\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=10, n_estimators=5000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=10, n_estimators=5000, total= 1.2min\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=10, n_estimators=5000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=10, n_estimators=5000, total= 1.2min\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=10, n_estimators=5000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=10, n_estimators=5000, total= 1.2min\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=10, n_estimators=5000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=10, n_estimators=5000, total= 1.2min\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=20, n_estimators=100 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=20, n_estimators=100, total=   1.4s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=20, n_estimators=100 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=20, n_estimators=100, total=   1.3s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=20, n_estimators=100 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=20, n_estimators=100, total=   1.3s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=20, n_estimators=100 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=20, n_estimators=100, total=   1.3s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=20, n_estimators=100 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=20, n_estimators=100, total=   1.4s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=20, n_estimators=1000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=20, n_estimators=1000, total=  14.0s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=20, n_estimators=1000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=20, n_estimators=1000, total=  13.9s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=20, n_estimators=1000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=20, n_estimators=1000, total=  13.8s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=20, n_estimators=1000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=20, n_estimators=1000, total=  14.1s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=20, n_estimators=1000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=20, n_estimators=1000, total=  14.1s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=20, n_estimators=5000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=20, n_estimators=5000, total= 1.2min\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=20, n_estimators=5000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=20, n_estimators=5000, total= 1.2min\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=20, n_estimators=5000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=20, n_estimators=5000, total= 1.2min\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=20, n_estimators=5000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=20, n_estimators=5000, total= 1.2min\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=20, n_estimators=5000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=5, min_samples_split=20, n_estimators=5000, total= 1.2min\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=2, n_estimators=100 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=2, n_estimators=100, total=   2.4s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=2, n_estimators=100 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=2, n_estimators=100, total=   2.4s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=2, n_estimators=100 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=2, n_estimators=100, total=   2.4s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=2, n_estimators=100 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=2, n_estimators=100, total=   2.4s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=2, n_estimators=100 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=2, n_estimators=100, total=   2.4s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=2, n_estimators=1000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=2, n_estimators=1000, total=  26.9s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=2, n_estimators=1000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=2, n_estimators=1000, total=  26.7s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=2, n_estimators=1000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=2, n_estimators=1000, total=  26.6s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=2, n_estimators=1000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=2, n_estimators=1000, total=  26.9s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=2, n_estimators=1000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=2, n_estimators=1000, total=  27.0s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=2, n_estimators=5000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=2, n_estimators=5000, total= 2.3min\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=2, n_estimators=5000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=2, n_estimators=5000, total= 2.2min\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=2, n_estimators=5000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=2, n_estimators=5000, total= 2.2min\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=2, n_estimators=5000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=2, n_estimators=5000, total= 2.3min\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=2, n_estimators=5000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=2, n_estimators=5000, total= 2.3min\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=10, n_estimators=100 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=10, n_estimators=100, total=   2.4s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=10, n_estimators=100 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=10, n_estimators=100, total=   2.4s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=10, n_estimators=100 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=10, n_estimators=100, total=   2.4s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=10, n_estimators=100 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=10, n_estimators=100, total=   2.4s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=10, n_estimators=100 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=10, n_estimators=100, total=   2.4s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=10, n_estimators=1000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=10, n_estimators=1000, total=  27.0s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=10, n_estimators=1000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=10, n_estimators=1000, total=  26.7s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=10, n_estimators=1000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=10, n_estimators=1000, total=  26.4s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=10, n_estimators=1000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=10, n_estimators=1000, total=  26.8s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=10, n_estimators=1000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=10, n_estimators=1000, total=  27.0s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=10, n_estimators=5000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=10, n_estimators=5000, total= 2.3min\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=10, n_estimators=5000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=10, n_estimators=5000, total= 2.2min\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=10, n_estimators=5000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=10, n_estimators=5000, total= 2.2min\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=10, n_estimators=5000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=10, n_estimators=5000, total= 2.3min\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=10, n_estimators=5000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=10, n_estimators=5000, total= 2.3min\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=20, n_estimators=100 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=20, n_estimators=100, total=   2.4s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=20, n_estimators=100 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=20, n_estimators=100, total=   2.4s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=20, n_estimators=100 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=20, n_estimators=100, total=   2.3s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=20, n_estimators=100 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=20, n_estimators=100, total=   2.4s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=20, n_estimators=100 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=20, n_estimators=100, total=   2.4s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=20, n_estimators=1000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=20, n_estimators=1000, total=  27.0s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=20, n_estimators=1000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=20, n_estimators=1000, total=  26.8s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=20, n_estimators=1000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=20, n_estimators=1000, total=  26.7s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=20, n_estimators=1000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=20, n_estimators=1000, total=  26.8s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=20, n_estimators=1000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=20, n_estimators=1000, total=  27.3s\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=20, n_estimators=5000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=20, n_estimators=5000, total= 2.3min\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=20, n_estimators=5000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=20, n_estimators=5000, total= 2.3min\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=20, n_estimators=5000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=20, n_estimators=5000, total= 2.2min\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=20, n_estimators=5000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=20, n_estimators=5000, total= 2.3min\n",
            "[CV] learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=20, n_estimators=5000 \n",
            "[CV]  learning_rate=0.05, loss=ls, max_depth=10, min_samples_split=20, n_estimators=5000, total= 2.3min\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=2, n_estimators=100 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=2, n_estimators=100, total=   1.0s\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=2, n_estimators=100 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=2, n_estimators=100, total=   1.0s\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=2, n_estimators=100 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=2, n_estimators=100, total=   1.0s\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=2, n_estimators=100 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=2, n_estimators=100, total=   1.0s\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=2, n_estimators=100 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=2, n_estimators=100, total=   1.0s\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=2, n_estimators=1000 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=2, n_estimators=1000, total=  10.3s\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=2, n_estimators=1000 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=2, n_estimators=1000, total=  10.3s\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=2, n_estimators=1000 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=2, n_estimators=1000, total=  10.2s\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=2, n_estimators=1000 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=2, n_estimators=1000, total=  10.3s\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=2, n_estimators=1000 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=2, n_estimators=1000, total=  10.4s\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=2, n_estimators=5000 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=2, n_estimators=5000, total=  51.6s\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=2, n_estimators=5000 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=2, n_estimators=5000, total=  51.2s\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=2, n_estimators=5000 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=2, n_estimators=5000, total=  50.7s\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=2, n_estimators=5000 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=2, n_estimators=5000, total=  51.0s\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=2, n_estimators=5000 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=2, n_estimators=5000, total=  51.1s\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=10, n_estimators=100 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=10, n_estimators=100, total=   1.0s\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=10, n_estimators=100 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=10, n_estimators=100, total=   1.0s\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=10, n_estimators=100 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=10, n_estimators=100, total=   1.0s\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=10, n_estimators=100 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=10, n_estimators=100, total=   1.0s\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=10, n_estimators=100 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=10, n_estimators=100, total=   1.0s\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=10, n_estimators=1000 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=10, n_estimators=1000, total=  10.3s\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=10, n_estimators=1000 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=10, n_estimators=1000, total=  10.2s\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=10, n_estimators=1000 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=10, n_estimators=1000, total=  10.2s\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=10, n_estimators=1000 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=10, n_estimators=1000, total=  10.2s\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=10, n_estimators=1000 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=10, n_estimators=1000, total=  10.3s\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=10, n_estimators=5000 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=10, n_estimators=5000, total=  51.1s\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=10, n_estimators=5000 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=10, n_estimators=5000, total=  50.7s\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=10, n_estimators=5000 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=10, n_estimators=5000, total=  50.6s\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=10, n_estimators=5000 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=10, n_estimators=5000, total=  51.3s\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=10, n_estimators=5000 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=10, n_estimators=5000, total=  51.4s\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=20, n_estimators=100 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=20, n_estimators=100, total=   1.0s\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=20, n_estimators=100 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=20, n_estimators=100, total=   1.0s\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=20, n_estimators=100 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=20, n_estimators=100, total=   1.0s\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=20, n_estimators=100 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=20, n_estimators=100, total=   1.1s\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=20, n_estimators=100 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=20, n_estimators=100, total=   1.1s\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=20, n_estimators=1000 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=20, n_estimators=1000, total=  10.3s\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=20, n_estimators=1000 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=20, n_estimators=1000, total=  10.2s\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=20, n_estimators=1000 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=20, n_estimators=1000, total=  10.2s\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=20, n_estimators=1000 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=20, n_estimators=1000, total=  10.3s\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=20, n_estimators=1000 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=20, n_estimators=1000, total=  10.3s\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=20, n_estimators=5000 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=20, n_estimators=5000, total=  51.3s\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=20, n_estimators=5000 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=20, n_estimators=5000, total=  50.9s\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=20, n_estimators=5000 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=20, n_estimators=5000, total=  50.7s\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=20, n_estimators=5000 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=20, n_estimators=5000, total=  51.5s\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=20, n_estimators=5000 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=3, min_samples_split=20, n_estimators=5000, total=  51.3s\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=2, n_estimators=100 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=2, n_estimators=100, total=   1.8s\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=2, n_estimators=100 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=2, n_estimators=100, total=   1.7s\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=2, n_estimators=100 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=2, n_estimators=100, total=   1.7s\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=2, n_estimators=100 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=2, n_estimators=100, total=   1.7s\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=2, n_estimators=100 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=2, n_estimators=100, total=   1.8s\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=2, n_estimators=1000 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=2, n_estimators=1000, total=  16.6s\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=2, n_estimators=1000 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=2, n_estimators=1000, total=  16.8s\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=2, n_estimators=1000 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=2, n_estimators=1000, total=  16.6s\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=2, n_estimators=1000 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=2, n_estimators=1000, total=  16.6s\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=2, n_estimators=1000 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=2, n_estimators=1000, total=  16.6s\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=2, n_estimators=5000 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=2, n_estimators=5000, total= 1.4min\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=2, n_estimators=5000 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=2, n_estimators=5000, total= 1.4min\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=2, n_estimators=5000 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=2, n_estimators=5000, total= 1.3min\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=2, n_estimators=5000 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=2, n_estimators=5000, total= 1.4min\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=2, n_estimators=5000 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=2, n_estimators=5000, total= 1.4min\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=10, n_estimators=100 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=10, n_estimators=100, total=   1.8s\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=10, n_estimators=100 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=10, n_estimators=100, total=   1.7s\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=10, n_estimators=100 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=10, n_estimators=100, total=   1.7s\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=10, n_estimators=100 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=10, n_estimators=100, total=   1.7s\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=10, n_estimators=100 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=10, n_estimators=100, total=   1.8s\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=10, n_estimators=1000 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=10, n_estimators=1000, total=  16.6s\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=10, n_estimators=1000 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=10, n_estimators=1000, total=  16.7s\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=10, n_estimators=1000 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=10, n_estimators=1000, total=  16.4s\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=10, n_estimators=1000 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=10, n_estimators=1000, total=  16.8s\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=10, n_estimators=1000 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=10, n_estimators=1000, total=  16.7s\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=10, n_estimators=5000 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=10, n_estimators=5000, total= 1.4min\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=10, n_estimators=5000 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=10, n_estimators=5000, total= 1.3min\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=10, n_estimators=5000 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=10, n_estimators=5000, total= 1.3min\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=10, n_estimators=5000 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=10, n_estimators=5000, total= 1.4min\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=10, n_estimators=5000 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=10, n_estimators=5000, total= 1.4min\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=20, n_estimators=100 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=20, n_estimators=100, total=   1.8s\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=20, n_estimators=100 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=20, n_estimators=100, total=   1.7s\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=20, n_estimators=100 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=20, n_estimators=100, total=   1.7s\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=20, n_estimators=100 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=20, n_estimators=100, total=   1.7s\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=20, n_estimators=100 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=20, n_estimators=100, total=   1.7s\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=20, n_estimators=1000 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=20, n_estimators=1000, total=  16.4s\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=20, n_estimators=1000 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=20, n_estimators=1000, total=  16.3s\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=20, n_estimators=1000 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=20, n_estimators=1000, total=  16.2s\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=20, n_estimators=1000 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=20, n_estimators=1000, total=  16.4s\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=20, n_estimators=1000 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=20, n_estimators=1000, total=  16.5s\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=20, n_estimators=5000 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=20, n_estimators=5000, total= 1.4min\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=20, n_estimators=5000 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=20, n_estimators=5000, total= 1.3min\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=20, n_estimators=5000 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=20, n_estimators=5000, total= 1.3min\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=20, n_estimators=5000 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=20, n_estimators=5000, total= 1.3min\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=20, n_estimators=5000 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=5, min_samples_split=20, n_estimators=5000, total= 1.4min\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=10, min_samples_split=2, n_estimators=100 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=10, min_samples_split=2, n_estimators=100, total=   5.0s\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=10, min_samples_split=2, n_estimators=100 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=10, min_samples_split=2, n_estimators=100, total=   4.9s\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=10, min_samples_split=2, n_estimators=100 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=10, min_samples_split=2, n_estimators=100, total=   4.8s\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=10, min_samples_split=2, n_estimators=100 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=10, min_samples_split=2, n_estimators=100, total=   4.9s\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=10, min_samples_split=2, n_estimators=100 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=10, min_samples_split=2, n_estimators=100, total=   5.0s\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=10, min_samples_split=2, n_estimators=1000 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=10, min_samples_split=2, n_estimators=1000, total=  36.0s\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=10, min_samples_split=2, n_estimators=1000 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=10, min_samples_split=2, n_estimators=1000, total=  34.9s\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=10, min_samples_split=2, n_estimators=1000 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=10, min_samples_split=2, n_estimators=1000, total=  35.8s\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=10, min_samples_split=2, n_estimators=1000 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=10, min_samples_split=2, n_estimators=1000, total=  36.0s\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=10, min_samples_split=2, n_estimators=1000 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=10, min_samples_split=2, n_estimators=1000, total=  36.1s\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=10, min_samples_split=2, n_estimators=5000 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=10, min_samples_split=2, n_estimators=5000, total= 2.8min\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=10, min_samples_split=2, n_estimators=5000 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=10, min_samples_split=2, n_estimators=5000, total= 2.7min\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=10, min_samples_split=2, n_estimators=5000 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=10, min_samples_split=2, n_estimators=5000, total= 2.7min\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=10, min_samples_split=2, n_estimators=5000 \n",
            "[CV]  learning_rate=0.05, loss=lad, max_depth=10, min_samples_split=2, n_estimators=5000, total= 2.7min\n",
            "[CV] learning_rate=0.05, loss=lad, max_depth=10, min_samples_split=2, n_estimators=5000 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jv5UcjMLtvQU"
      },
      "source": [
        "# print best parameter configuration and best score \n",
        "print(clf.best_params_, clf.best_score_)\n",
        "best_params = clf.best_params"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlbvzMW6tvQV"
      },
      "source": [
        "# save best reference model\n",
        "joblib.dump(clf, './models/final_model_gridsearch.pkl')\n",
        "joblib.dump(clf.best_estimator_, './models/final_model_best_estimator.pkl');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jGw6pNxtvQV"
      },
      "source": [
        "# print full estimator configuration\n",
        "clf.best_estimator_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fE3ot_B5tvQW"
      },
      "source": [
        "best_params = {'learning_rate': 0.05, 'loss': 'huber', 'max_depth': 5, 'min_samples_split': 20, 'n_estimators': 100}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzB5gqdytvQW"
      },
      "source": [
        "# 3. Analysis of final model performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4RuqBnetvQW"
      },
      "source": [
        "The best parameter combination of the grid search is depicted above. Since no explicit test set has been withdrawn from the data before grid searching, we have to rerun the model with the best parameter combination again on a train-test split of the data. Although the grid search applies a three fold cross validation, the final best model has been fit to all data passed to the grid search instance. As a result, the model might be overfitted to the data. To exclude the possibility of overfitting, we will retrain the model, this time keeping an explicit unseen test set for the final evaluation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "orKM1HmhtvQW"
      },
      "source": [
        "# split data into training and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(features[cols7], labels, test_size=0.2, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlusAFnEtvQW"
      },
      "source": [
        "# mean absolute error of the fitted estimator from the grid search run\n",
        "print(mean_absolute_error(y_test, clf.best_estimator_.predict(X_test)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "70EPZmK0tvQW"
      },
      "source": [
        "# re-training of the model with the specified best parameter combination\n",
        "apply_predictor(GradientBoostingRegressor(**best_params), features[cols7], labels.squeeze(), 'Final model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fda0DmqxtvQX"
      },
      "source": [
        "**Analysis:**\n",
        "\n",
        "As we can see above, the performance on **in-sample data** is significantly better than for **out-of-sample data**. Using the grid search estimator, we can achieve a mean absolute error of 3.11, while the performance on an unseen test set is around 3.46 mean absolute error.\n",
        "\n",
        "The **final representative model performance** can be summarized as follows: The yearly average out of sample absolute error lies at 3.46€ per MWh. This refers to a root mean square error of 4.72€ per MWh and a median absolute percentage error of 8.39%. \n",
        "\n",
        "---\n",
        "\n",
        "## 3.1 Reference benchmarks - Margin of improvement\n",
        "\n",
        "Below, some reference points for the final model are given. First, the performance of a model with perfect forecasting information for renewable generation is given. This represents a benchmark for what we would be able to achieve with the _chosen modeling approach_ at best. Ideally, the model would be based on a weather forecast which is used to predict the renewable generation forecast, as explained above.\n",
        "\n",
        "Second, a reference point for a model with total perfect information about all generation types is given. This benchmark represents a best case scenario in which we would have perfect information about all generation input parameters, togehter with the time series bottleneck features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3hf3hZMtvQX"
      },
      "source": [
        "best_params['n_estimators']=1000\n",
        "apply_predictor(GradientBoostingRegressor(**best_params), features[cols6], labels.squeeze(), 'Benchmark model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lB6qiQ8ftvQX"
      },
      "source": [
        "best_params['n_estimators']=5000\n",
        "apply_predictor(GradientBoostingRegressor(**best_params), features[cols4], labels.squeeze(), 'Perfect information model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXGa46OOtvQX"
      },
      "source": [
        "As we can see, there is quite a margin of improvement towards the first and second benchmark. Better renewable forecasts would be able to improve the overall model performance quite signigicantly. The second benchmark shows, that additional effort for generating other input features, like brown or hard coal generation prediction, has the potential to improve the model even further. This should be considered in possible future steps to improve the model.\n",
        "\n",
        "---\n",
        "\n",
        "## 3.2 Model evaluation on full-month out of sample data\n",
        "\n",
        "Below, the final model performance is evaluated on a monthly test set. Instead of picking a random test set of the 2015 data, we use one complete month at a time as a test set. In recent literature about Day-Ahead price forecasting, model performance is often evaluated on a monthly or even weekly basis. Since there are 'easy' and 'hard' month in a year, this is only comparable to our results if we provide some 'best' and 'worst' case prediction performance metrics. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "miYcq_28tvQY"
      },
      "source": [
        "def apply_predictor_monthly(predictor, features, labels, month, modelname='Some model'):\n",
        "        \n",
        "    \"\"\"Perform a train-test split with a full month as test data. Fit a predictor on\n",
        "    the training set and evaluate error metric on the test\n",
        "    set.\"\"\"\n",
        "    \n",
        "    # pick indices for one month only\n",
        "    index = features[features.index.month==month].index\n",
        "    X_test, y_test = features.loc[index], labels.loc[index]\n",
        "    X_train, y_train = features.drop(index), labels.drop(index)\n",
        "\n",
        "    # fit predictor and predict on test set\n",
        "    predictor.fit(X_train, y_train)\n",
        "    pred = predictor.predict(X_test)\n",
        "    \n",
        "    # compute performance metrics\n",
        "    mae = mean_absolute_error(y_test, pred)\n",
        "    mse = mean_squared_error(y_test, pred)\n",
        "    med = median_absolute_error(y_test, pred)\n",
        "    medape = np.median(np.abs((y_test - pred) / y_test) * 100)\n",
        "    rmse = np.sqrt(mse)\n",
        "    max_miss = max(abs(y_test - pred))\n",
        "    r2 = r2_score(y_test, pred)\n",
        "    \n",
        "    print('{7:s} mean absolute error: {0:.2f}, median absolute error: {1:.2f}, mean squared error {2:.2f}, '\n",
        "      'root mean squared error: {3:.2f}, median absolute percentage error: {4:.2f},  maximum deviation: {5:.2f},'\\\n",
        "      'r2 score: {6:.2f}'\\\n",
        "      .format(mae, med, mse, rmse, medape, max_miss, r2, modelname))\n",
        "    \n",
        "    return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jYkahmhtvQY"
      },
      "source": [
        "# re-training of the model with the specified best parameter combination\n",
        "best_params['n_estimators']=100\n",
        "for m in range(1, 13, 1):\n",
        "    print('Results for month {}'.format(m))\n",
        "    apply_predictor_monthly(GradientBoostingRegressor(**best_params), features[cols7], labels.squeeze(), m, 'Final model')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
